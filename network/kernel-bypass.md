# 高性能网络处理：从 DPDK 到 XDP

## Linux 内核是性能瓶颈所在

从前面内核网络优化的章节描述中，可以总结出 Linux 内核网络协议栈为基础应用方案存在几个问题：

- 应用程序和网络协议栈的交互过程中存在用户态和内核态的频繁切换，操作系统在执行此类操作时，会涉及当前进程上下文切换，TLB也会被频繁更新，导致 MMU 需要经常访问页表，这些都影响数据收发的时延。
- 用户空间缓存和内核空间缓存之间的复制行为，也消耗了大量的时间。
- 另外内核协议栈对数据的各种封包、解包也会消耗CPU 时钟。

以上几个缺点都会提高网络包的整体时延，特别是在云计算网络核心节点，这些环境通常有非常高的并发数据量，云架构中复杂的服务治理已经将C10K的问题发展到C10M（单机并发 1000 万）。一个大规模集群内，面对东西向规模性的Gbp/s数据流量，关键节点的挑战是用户态协议栈和多核并发问题。

这个时候，Linux 网络协议栈进行的各种优化策略，基本都没有太大效果。网络协议栈的冗长流程才是最主要的性能负担。


不过技术总在发展，业界专家已经想到了很多新的技术来解决上述问题，这些新技术包括 DPDK、RDMA、XDP等。

## DPDK

数据平面开发工具包 （data plane development kit， DPDK） 是在用户态运行的一组软件库和驱动程序，可在大部分主要的 CPU 体系结构上加速网络数据包的处理。作为 Linux 基金会下的开源项目， DPDK 在推动通用 CPU 在高性能网络环境中发挥了很大的作用。

### DPDK 技术概述

传统的网络设备（NIC），在执行底层数据平面功能，比如数据包的转发和路由，使用的都是专用的集成电路芯片(ASIC), 以 ASIC 芯片 + 配套软件的产品架构提供的吞吐量也能达到高性能网络的要求，但其新产品推出收到芯片开发周期的限制，而且 ASIC 供应商之间也不存在软件移植的可能。

后续 Intel、Cavium等半导体公司开始将通用多核处理器引入网络数据处理领域，进行底层的数据包处理，这些方案再处理器性能和成本方面可以和 ASIC 芯片的网络产品竞争，但问题在于，以 Linux 内核协议栈为基础的网络方案存在许多瓶颈，无法高性能地处理数据包。

此时需要一个解决方案来消除这些瓶颈，同时还要保持原有 Linux 程序的兼容性，新方案最好还要以库的形式打包到 Linux 发行版中，在用户需要时来管理各种网络设备。

这些目标随着 Intel 基于Nehalem微架构的 Xeon 处理器推出的 DPDK 而实现，DPDK 绕过（bypass） Linux 内核，在用户态执行数据包处理，以提供尽可能的网络性能。

DPDK 程序运行在操作系统的用户态，利用自带的数据平台库进行数据包的发送、处理和接收、绕过运行在内核态的网络协议栈，大幅提升数据包的处理效率。

DPDK 的网卡驱动程序运行在用户态，屏蔽了网卡硬件发起的大部分中断，采用主动轮询的方式持续检查网卡的接受/发送队列，查看是否有新数据到达或者是否可以继续发送数据，而从实现高吞吐量和低时延，因为 DPDK的驱动程序 也被称为 轮询模式驱动程序 (poll mode driver)。


<div  align="center">
	<img src="../assets/dpdk.png" width = "550"  align=center />
</div>

图：DPDK 与 传统内核网络的对比
- 左边是原来的方式数据从 网卡 -> 驱动 -> 协议栈 -> Socket接口 -> 业务
- 右边是DPDK的方式，基于UIO（Userspace I/O）旁路数据。数据从 网卡 -> DPDK轮询模式-> DPDK基础库 -> 业务


不过 Linux 内核 仍然是 DPDK 实现的基础，比如 内核中的 UIO 驱动框架，它为 DPDK 驱动程序 提供获取寄存器的地址、中断计数等功能， 另外 内核提供的大页 机制，也是 DPDK 进行内存管理的基础。

### DPDK 性能指标

以下为 DPVS (LVS的DPDK优化版本)与 传统 LVS 在 PPS 转发上的指标对比，性能提升约 300%；

<div  align="center">
	<img src="../assets/dpvs-performance.png" width = "550"  align=center />
</div>


## XDP

DPDK 内核旁路技术现在也逐渐成为网络处理加速的一种成熟方案，然而这种方案在 Linux 看来有它的缺陷：独立于 Linux 内核。在 2016 年 Linux Netdev 会议上，David S. Miller 更是让观众和他一起念 “DPDK is not Linux”。

同年，伴随着 eBPF 技术的成熟，Linux 也终于合入了属于自己的网络处理高速公路：XDP。

XDP（eXpress Data Path）是 Linux 内核中提供高性能、可编程的网络数据包处理框架，本质上是 Linux内核网络模块中的一个 BPF Hook，能够动态挂载 eBPF 程序逻辑，使得 Linux 内核能够在数据报文到达 L2（网卡驱动层）时就对其进行针对性的高速处理，而无需再 “循规蹈矩” 地进入到 TCP/IP 协议栈。

也就是说 XDP 的基础是 BPF，那么要学习 XDP，就要先知道什么是 BPF。 

BPF 是 Berkeley Packet Filter 的缩写，原本是一个很冷门的技术，作用主要是提供网络数据包过滤功能的性能。后续经过一些列的完善，在 2014年 并入 Linux 内核主线，从此 BPF 变成了一个更通用的执行引擎，可以完成多种工作，比如性能分析、当然也包括它的老本行 - 网络数据包处理。

简单来说， BPF 技术提供了一种在各种内核事件和应用程序事件发生时运行一段小程序的机制，该技术将内核变得完全可编程，允许开发者定制、控制他们的系统，以解决上层应用问题。

目前 BPF 的主要应用方向分别是 网络、观测系统和安全。网络领域的应用也就是 XDP， 也就是说 XDP 程序是一种 BPF 程序。


### XDP 数据处理

下面展示了 XDP 程序执行的流程， 网卡驱动调用内核 XDP 模块提供的 API ，然后间接调用用户提供的XDP程序，也就是说 XDP 是在设备驱动程序的上下文中执行。
<div  align="center">
	<img src="../assets/XDP.png" width = "350"  align=center />
</div>

在网卡接收到数据包之后， 内核协议栈处理数据包之前，设备驱动程序会先调用 XDP 模块中的 BPF 程序，这段程序进行以下几种工作

- XDP_DROP 丢弃数据包 
- XDP_REDIRECT 将数据包重定向其他网络接口（包括虚拟网卡），或者结合 AF_XDP 套接字重定向用户态程序。
- XDP_PASS 放行数据包，数据包进入常规的内核网络协议栈


目前，XDP 已经是 Linux 内核的一部分，与现有的内核网络协议栈完全兼容，二者可以协同工作， 现在 XDP 在如网络处理 Cilium 方案、 Envoy 网络加速或者观测性方面，也已经成熟落地，未来也将必有更大的发展。
